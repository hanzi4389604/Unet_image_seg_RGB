{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#import cv2.cv2 as cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "#from dataset.dataset import MyDataSet\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import BCELoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "from net.nets import UNet\n",
    "from setting.setting import best_params_path, last_params_path, train_log_dir, is_save_img\n",
    "from setting.setting import img_size\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "class MyDataSet(Dataset):\n",
    "    def __init__(self, data_dir=voc_data_dir):\n",
    "        self.train_dir = os.path.join(data_dir, 'JPEGImages')\n",
    "        self.target_dir = os.path.join(data_dir, 'SegmentationClass')\n",
    "        self.dataset = os.listdir(self.target_dir)\n",
    "        print('see',self.dataset[0])\n",
    "        self.trans = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, aaa):\n",
    "        img_name = self.dataset[aaa]\n",
    "#        print(img_name)\n",
    "#        print(img_name[0:-2])\n",
    "        train_img = Image.open(os.path.join(self.train_dir, f'{img_name[:-3]}jpg'))  # 训练图片\n",
    "#        print(train_img)\n",
    "        target_img = Image.open(os.path.join(self.target_dir, img_name))  # 分割标签\n",
    "#        print(target_img)\n",
    "        \"\"\" 统一尺寸 \"\"\"\n",
    "        w, h = train_img.size\n",
    "#        print(w,h)\n",
    "        max_border = max(w, h)\n",
    "#        print(max_border)\n",
    "        train_img = train_img.resize((int(img_size*w/max_border), int(img_size*h/max_border)))\n",
    "#        print(train_img.size)\n",
    "#        print(train_img)\n",
    "#        print(img_size,w,max_border)\n",
    "#        print(img_size*w/max_border)\n",
    "        target_img = target_img.resize((int(img_size*w/max_border), int(img_size*h/max_border)))\n",
    "\n",
    "        \"\"\" 创建数据和标签 \"\"\"\n",
    "        train_img_back = transforms.ToPILImage()(torch.zeros((3, img_size, img_size)))\n",
    "        target_img_back = transforms.ToPILImage()(torch.zeros((3, img_size, img_size)))\n",
    "        train_img_back.paste(train_img)\n",
    "        target_img_back.paste(target_img)\n",
    "#        plt.imshow(train_img_back)\n",
    "#        plt.show()\n",
    "#        plt.imshow(target_img_back)\n",
    "#        plt.show()\n",
    "        return self.trans(train_img_back), self.trans(target_img_back)\n",
    "\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    train_dataset = MyDataSet(r'/home/l/20211218 practice/data/unet4/unet_voc-master/dataset/VOCdevkit/VOC2007')\n",
    "#    print(train_dataset[0])\n",
    "#    print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matlab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e7daa0055a10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatlab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matlab'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "see 009841.png\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m MyDataSet()\n\u001b[1;32m      7\u001b[0m     train_dataLoader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 8\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mUNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#    if os.path.exists(best_params_path):\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#        model.load_state_dict(torch.load(best_params_path))\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#        print('init params load success')\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#    else:\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#        print('no params to init')\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     loss_fun \u001b[38;5;241m=\u001b[39m BCELoss()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:689\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    674\u001b[0m \n\u001b[1;32m    675\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 689\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:602\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 602\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:689\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    674\u001b[0m \n\u001b[1;32m    675\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 689\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "batch_size = 12\n",
    "lr = 0.0001\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    summary_writer = SummaryWriter(log_dir=train_log_dir)\n",
    "    train_dataset = MyDataSet()\n",
    "    train_dataLoader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    model = UNet().cuda()\n",
    "#    if os.path.exists(best_params_path):\n",
    "#        model.load_state_dict(torch.load(best_params_path))\n",
    "#        print('init params load success')\n",
    "#    else:\n",
    "#        print('no params to init')\n",
    "    loss_fun = BCELoss()\n",
    "    opt = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    epoch = 0\n",
    "    global_step = 0\n",
    "    best_params = None\n",
    "    while True:\n",
    "        epoch += 1\n",
    "        epoch_loss = []\n",
    "\n",
    "        model.train()\n",
    "        for batch_index, (x, y) in enumerate(train_dataLoader):\n",
    "            batch_index += 1\n",
    "            global_step += 1\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "        #    print('shape x',x.shape)\n",
    "        #    print('shape y',y.shape)\n",
    "            y_pre = model(x)\n",
    "        #    print('shape y_pre',y_pre.shape)\n",
    "            loss = loss_fun(y_pre, y)\n",
    "            #print(loss)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            print(f'epoch: {epoch}/{batch_index}  loss ==> {loss.item()}')\n",
    "            summary_writer.add_scalar('loss', loss.item(), global_step=global_step)\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "            if is_save_img:\n",
    "                \"\"\" 保存图片查看效果 \"\"\"\n",
    "                img_list = []\n",
    "                \n",
    "                for i in range(x.shape[0]):\n",
    "                    \n",
    "                    img_list.append(torch.stack((x[i], y[i], y_pre[i]), dim=0))\n",
    "                    \n",
    "                img_list = torch.cat(img_list, dim=0)\n",
    "      #          print('img_list shape',img_list.shape)\n",
    "                img_save_dir = f'img/epoch={epoch}'\n",
    "                if not os.path.exists(img_save_dir):\n",
    "                    os.makedirs(img_save_dir)\n",
    "                save_image(img_list, os.path.join(img_save_dir, f'{batch_index}.png'))\n",
    "\n",
    "        \"\"\" 模型保存 \"\"\"\n",
    "        if epoch == 1:\n",
    "            best_params = np.mean(epoch_loss)\n",
    "        elif np.mean(epoch_loss) < best_params:\n",
    "            best_params = np.mean(epoch_loss)\n",
    "            torch.save(model.state_dict(), best_params_path)\n",
    "            print('best model save success')\n",
    "        \n",
    "        ############need to be debug\n",
    "        \n",
    "        #torch.save(model.state_dict(), last_params_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/l/20211218 practice/data/20220528_mask_rCNN/unet_voc-master/dataset/VOCdevkit/VOC2007/static_resources/tensorboard_log/train_log\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "BASEDIR = r'/home/l/20211218 practice/data/20220528_mask_rCNN/unet_voc-master/dataset/VOCdevkit/VOC2007'\n",
    "voc_data_dir = r'/home/l/20211218 practice/data/20220528_mask_rCNN/unet_voc-master/dataset/VOCdevkit/VOC2007'\n",
    "\n",
    "img_size = 224\n",
    "\n",
    "best_params_path = os.path.join(BASEDIR, 'static_resources/save_model/best_params')\n",
    "#print(best_params_path)\n",
    "last_params_path = os.path.join(BASEDIR, 'static_resources/save_model/last_params')\n",
    "train_log_dir = os.path.join(BASEDIR, 'static_resources/tensorboard_log/train_log')\n",
    "print(train_log_dir)\n",
    "is_save_img = True  # 是否保存图片进行查看"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
